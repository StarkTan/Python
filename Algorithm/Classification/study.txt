常用分类算法学习：
  参考网站：https://blog.csdn.net/songguangfan/article/details/92581643
  1.NBC算法（Naive Bayesian Classifier，朴素贝叶斯分类）
    简介与原理：
      NBC 模型发源于古典数学理论，有着坚实的数学基础。
      该算法是基于条件独立性假设的一种算法，当条件独立性假设成立时，利用贝叶斯公式计算出其后验概率，
      即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。
    优点：
      NBC 算法逻辑简单，易于实现；
      NBC 算法所需估计的参数很少；
      NBC 算法对缺失数据不太敏感；
      NBC 算法具有较小的误差分类率；
      NBC 算法性能稳定，健壮性比较好；
    缺点：
      在属性个数比较多或者属性之间相关性较大时，NBC 模型的分类效果相对较差；
      算法是基于条件独立性假设的，在实际应用中很难成立，故会影响分类效果

  2.LR算法（Logistic Regress，逻辑回归）
    简介与原理
      LR 回归是当前业界比较常用的机器学习方法，用于估计某种事物的可能性。它与多元线性回归同属一个家族，即广义线性模型。
      简单来说多元线性回归是直接将特征值和其对应的概率进行相乘得到一个结果，逻辑回归则是在这样的结果上加上一个逻辑函数。
    优点：
      对数据中小噪声的鲁棒性好；
      LR 算法已被广泛应用于工业问题中；
      多重共线性并不是问题，它可结合正则化来解决。
    缺点：
      对于非线性特征，需要转换
      当特征空间很大时，LR的性能并不是太好