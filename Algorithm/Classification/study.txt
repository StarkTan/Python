常用分类算法学习：
  参考网站：https://blog.csdn.net/songguangfan/article/details/92581643
  1.NBC算法（Naive Bayesian Classifier，朴素贝叶斯分类）
    简介与原理：
      NBC 模型发源于古典数学理论，有着坚实的数学基础。
      该算法是基于条件独立性假设的一种算法，当条件独立性假设成立时，利用贝叶斯公式计算出其后验概率，
      即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。
    优点：
      NBC 算法逻辑简单，易于实现；
      NBC 算法所需估计的参数很少；
      NBC 算法对缺失数据不太敏感；
      NBC 算法具有较小的误差分类率；
      NBC 算法性能稳定，健壮性比较好；
    缺点：
      在属性个数比较多或者属性之间相关性较大时，NBC 模型的分类效果相对较差；
      算法是基于条件独立性假设的，在实际应用中很难成立，故会影响分类效果

  2.LR算法（Logistic Regress，逻辑回归）
    简介与原理
      LR 回归是当前业界比较常用的机器学习方法，用于估计某种事物的可能性。它与多元线性回归同属一个家族，即广义线性模型。
      简单来说多元线性回归是直接将特征值和其对应的概率进行相乘得到一个结果，逻辑回归则是在这样的结果上加上一个逻辑函数。
    优点：
      对数据中小噪声的鲁棒性好；
      LR 算法已被广泛应用于工业问题中；
      多重共线性并不是问题，它可结合正则化来解决。
    缺点：
      对于非线性特征，需要转换
      当特征空间很大时，LR的性能并不是太好

  3.SVM算法 （Support Vector Machine，支持向量机）
    简介与原理
      SVM 算法是建立在统计学习理论基础上的机器学习方法，为十大数据挖掘算法之一。
      通过学习算法，SVM 可以自动寻找出对分类有较好区分能力的支持向量，由此构造出的分类器可以最大化类与类的间隔，因而有较好的适应能力和较高的分准率。
      SVM 算法的目的在于寻找一个超平面H，该超平面可以将训练集中的数据分开，且与类域边界的沿垂直于该超平面方向的距离最大，故SVM 法亦被称为最大边缘算法。
    优点：
      SVM 模型有很高的分准率；
      SVM 模型有很高的泛化性能；
      SVM 模型能很好地解决高维问题；
      SVM 模型对小样本情况下的机器学习问题效果好。
    缺点：
      SVM 模型对缺失数据敏感
      对非线性问题没有通用解决方案，得谨慎选择核函数来处理